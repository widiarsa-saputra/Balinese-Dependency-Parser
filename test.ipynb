{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import fasttext\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5e79dc0bfa724be0a4365dcae84c7776-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">John</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">eats</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBZ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e79dc0bfa724be0a4365dcae84c7776-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e79dc0bfa724be0a4365dcae84c7776-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M220.0,179.0 L228.0,167.0 212.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e79dc0bfa724be0a4365dcae84c7776-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e79dc0bfa724be0a4365dcae84c7776-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e79dc0bfa724be0a4365dcae84c7776-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e79dc0bfa724be0a4365dcae84c7776-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Data custom\n",
    "data = {\n",
    "    \"words\": [\n",
    "        {\"text\": \"John\", \"tag\": \"NNP\"},\n",
    "        {\"text\": \"eats\", \"tag\": \"VBZ\"},\n",
    "        {\"text\": \"an\", \"tag\": \"DT\"},\n",
    "        {\"text\": \"apple\", \"tag\": \"NN\"},\n",
    "    ],\n",
    "    \"arcs\": [\n",
    "        {\"start\": 0, \"end\": 1, \"label\": \"nsubj\", \"dir\": \"right\"},\n",
    "        {\"start\": 1, \"end\": 3, \"label\": \"obj\", \"dir\": \"right\"},\n",
    "        {\"start\": 2, \"end\": 3, \"label\": \"det\", \"dir\": \"left\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Visualisasi dependensi\n",
    "displacy.render(data, style=\"dep\", manual=True, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeScorer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EdgeScorer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)         # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()                # Output: Probabilitas (0-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(word):\n",
    "    \"\"\"\n",
    "    fungsi ini digunakan untuk mendapatkan vektor embedding dari suatu kata\n",
    "    \"\"\"\n",
    "    return f'{list(emb_model.get_word_vector(word))}'\n",
    "\n",
    "def word_embedding_ns(word):\n",
    "    return emb_model.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_score(sentence, model):\n",
    "    sentence_split = sentence.split()\n",
    "    edge = []\n",
    "    scores = []\n",
    "\n",
    "    for index_i, word_i in enumerate(sentence_split):\n",
    "        for index_j, word_j in enumerate(sentence_split):\n",
    "            if index_i != index_j:\n",
    "                emb_i = word_embedding_ns(word_i)\n",
    "                emb_j = word_embedding_ns(word_j)\n",
    "                input_vec = torch.tensor(np.concatenate((emb_i, emb_j)), dtype=torch.float32).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    prob = model(input_vec).item()\n",
    "                edge.append([word_i, word_j])\n",
    "                scores.append(prob)\n",
    "    return scores, edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_score_map(model, sentence):\n",
    "    # Inisialisasi model\n",
    "    input_x = []\n",
    "\n",
    "    scores, edge = find_score(sentence, model)\n",
    "\n",
    "    for i in range(len(scores)):\n",
    "        input_x.append((edge[i][0], edge[i][1], scores[i]))\n",
    "\n",
    "    return input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_input(sentence):\n",
    "    input_dim = 200\n",
    "    hidden_dim = 128\n",
    "    model = EdgeScorer(input_dim, hidden_dim)\n",
    "    model.load_state_dict(torch.load('edge_scorer.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    input_x = edge_score_map(model, sentence)\n",
    "    return input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mst_parser(sentence):\n",
    "    edges = define_input(sentence)\n",
    "\n",
    "    # Membuat graf terarah\n",
    "    G = nx.DiGraph()\n",
    "    for u, v, weight in edges:\n",
    "        G.add_edge(u, v, weight=weight)\n",
    "\n",
    "    # Mencari MST menggunakan Chu-Liu/Edmonds\n",
    "    mst = nx.minimum_spanning_arborescence(G)\n",
    "\n",
    "    # Menampilkan hasil MST\n",
    "    # for u, v, weight in mst.edges(data=True):\n",
    "    #     print(f'{v} -> {u} dengan bobot {weight[\"weight\"]}')\n",
    "    return mst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ngalih', 'gegaene', 'gegaene', 'gegaene', 'gegaene', 'gegaene']"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in edge_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\widi arsa\\AppData\\Local\\Temp\\ipykernel_14980\\1659254594.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('edge_scorer.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ngalih', 'gegaene'),\n",
       " ('gegaene', 'daya'),\n",
       " ('gegaene', 'apanga'),\n",
       " ('gegaene', 'aluhan'),\n",
       " ('gegaene', 'Nangingke'),\n",
       " ('gegaene', 'ia')]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mst_parser(sentence).edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\widi arsa\\AppData\\Local\\Temp\\ipykernel_14980\\1659254594.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('edge_scorer.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Nangingke', 5],\n",
       " ['ia', 5],\n",
       " ['ngalih', 0],\n",
       " ['daya', 5],\n",
       " ['apanga', 5],\n",
       " ['gegaene', 2],\n",
       " ['aluhan', 5]]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_head(sentence, sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_head(sentence, sentence_split):\n",
    "    edge_score = list(mst_parser(sentence).edges)\n",
    "    \n",
    "    tail = [x[1] for x in edge_score]\n",
    "    dependence_word = []\n",
    "\n",
    "    for index, value in enumerate(sentence_split):\n",
    "        if value in tail:\n",
    "            head = edge_score[int(tail.index(value))]\n",
    "            dependence_word.append([value, head[0]])\n",
    "        else:\n",
    "            dependence_word.append([value])\n",
    "            \n",
    "    head_output = []\n",
    "\n",
    "    for dependence in dependence_word:\n",
    "        if len(dependence) == 1:\n",
    "            head_output.append([dependence[0], 0])\n",
    "        else:\n",
    "            head_output.append([dependence[0], sentence_split.index(dependence[1])])\n",
    "    return head_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'postag=' + postag,\n",
    "        'postag[:2]=' + postag[:2],\n",
    "        'index=' + str(i),\n",
    "        'head=' + str(sent[i][2])\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features_pos(sent, i):\n",
    "    \"\"\"\n",
    "    fungsi ini digunakan untuk membentuk feature input pada pos\n",
    "    sent => list kata yang berisikan [word]\n",
    "    i => index dari kata saat ini\n",
    "    model => merupakan model word_embedding yang dipanggil untuk mendapatkan vektor kata\n",
    "    \"\"\"\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias':1.0,\n",
    "        'word': word,\n",
    "        'emb_word': word_embedding(word), \n",
    "        'is_first' : i == 0,\n",
    "        'is_last' : i == len(sent)-1,\n",
    "        'is_title' : word[0].upper() == word[0],\n",
    "        'isupper' : word.upper() == word,\n",
    "        'islower' : word.lower() == word,\n",
    "        'prefix-1' : word[0],\n",
    "        'prefix-2' : word[:2],\n",
    "        'suffix-1' : word[-1],\n",
    "        'suffix-2' : word[-2:],\n",
    "\n",
    "        'prev_word-1': '' if i == 0 else (sent[i-1]),\n",
    "        'prev_word-1_prefix-1' : '' if i == 0 else (sent[i-1][0]),\n",
    "        'prev_word-1_prefix-2' : '' if i == 0 else (sent[i-1][:2]),\n",
    "        'prev_word-1_suffix-1' : '' if i == 0 else (sent[i-1][-1]),\n",
    "        'prev_word-1_suffix-2' : '' if i == 0 else (sent[i-1][-2:]),\n",
    "\n",
    "        'prev_word-2' : (sent[i-2][0]) if i > 1 else '',\n",
    "        'prev_word-2_prefix-1' : (sent[i-2][0]) if i > 1 else '',\n",
    "        'prev_word-2_prefix-2' : (sent[i-2][:2]) if i > 1 else '',\n",
    "        'prev_word-2_suffix-1' : (sent[i-2][-1]) if i > 1 else '',\n",
    "        'prev_word-2_suffix-2' : (sent[i-2][-2:]) if i > 1 else '',\n",
    "\n",
    "        'next_word-1' : '' if i == len(sent)-1 else (sent[i+1][0]),\n",
    "        'next_word-1_prefix-1' : '' if i == len(sent)-1 else (sent[i+1][0]),\n",
    "        'next_word-1_prefix-2' : '' if i == len(sent)-1 else (sent[i+1][:2]),\n",
    "        'next_word-1_suffix-1' : '' if i == len(sent)-1 else (sent[i+1][-1]),\n",
    "        'next_word-1_suffix-2' : '' if i == len(sent)-1 else (sent[i+1][-2:]),\n",
    "\n",
    "        'next_word-2' : (sent[i+2][0]) if i < len(sent)-2 else '',\n",
    "        'next_word-2_prefix-1' : (sent[i+2][0]) if i < len(sent)-2 else '',\n",
    "        'next_word-2_prefix-2' : (sent[i+2][:2]) if i < len(sent)-2 else '',\n",
    "        'next_word-2_suffix-1' : (sent[i+2][-1]) if i < len(sent)-2 else '',\n",
    "        'next_word-2_suffix-2' : (sent[i+2][-2:]) if i < len(sent)-2 else '',\n",
    "\n",
    "        'emb_prev_word-1': '' if i == 0 else word_embedding(sent[i-1][0]),\n",
    "        'emb_prev_word-1_prefix-2' : '' if i == 0 else word_embedding(sent[i-1][:2]),\n",
    "        'emb_prev_word-1_suffix-2' : '' if i == 0 else word_embedding(sent[i-1][-2:]),\n",
    "\n",
    "        'emb_prev_word-2' : word_embedding(sent[i-2][0]) if i > 1 else '',\n",
    "        'emb_prev_word-2_prefix-2' : word_embedding(sent[i-2][:2]) if i > 1 else '',\n",
    "        'emb_prev_word-2_suffix-2' : word_embedding(sent[i-2][-2:]) if i > 1 else '',\n",
    "\n",
    "        'emb_next_word-1' : '' if i == len(sent)-1 else word_embedding(sent[i+1][0]),\n",
    "\n",
    "        'emb_next_word-1_prefix-2' : '' if i == len(sent)-1 else word_embedding(sent[i+1][:2]),\n",
    "        'emb_next_word-1_suffix-2' : '' if i == len(sent)-1 else word_embedding(sent[i+1][-2:]),\n",
    "\n",
    "        'emb_next_word-2' : word_embedding(sent[i+2][0]) if i < len(sent)-2 else '',\n",
    "        'emb_next_word-2_prefix-2' : word_embedding(sent[i+2][:2]) if i < len(sent)-2 else '',\n",
    "        'emb_next_word-2_suffix-2' : word_embedding(sent[i+2][-2:]) if i < len(sent)-2 else '',\n",
    "\n",
    "    }\n",
    "                \n",
    "    return features\n",
    "\n",
    "def sent2features_pos(sent):\n",
    "    \"\"\"\n",
    "    fungsi ini merupakan fungsi untuk memanggil fungsi feature dan mengembalikan feature kata pada kalimat\n",
    "    \"\"\"\n",
    "    return [word2features_pos(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature2predict(sentence, pos_sentence):\n",
    "    feature = []\n",
    "    pos = pos_sentence\n",
    "    for index, word in enumerate(sentence.split()):\n",
    "        feature.append([word, pos[index]])\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_construct(sentence, pos_sentence):\n",
    "    head_features = construct_head(sentence, sentence.split())\n",
    "    features = make_feature2predict(sentence, pos_sentence)\n",
    "    for index, value in enumerate(features):\n",
    "        features[index].append(head_features[index][1])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('label_scorer.crfsuite')\n",
    "\n",
    "tagger_pos = pycrfsuite.Tagger()\n",
    "tagger_pos.open('person_balinese_pos_2.crfsuite')\n",
    "emb_model = fasttext.load_model('model_fasttext.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\widi arsa\\AppData\\Local\\Temp\\ipykernel_14980\\1659254594.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('edge_scorer.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nmod', 'subj', 'root', 'obj', 'iobj', 'xcomp', 'obj']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Nangingke ia ngalih daya apanga gegaene aluhan\"\n",
    "# output_label = tagger.tag(sent2features(feature_construct(sentence)))\n",
    "test_feature = sent2features_pos(sentence.split())\n",
    "pos_sentence = tagger_pos.tag(sent2features_pos(sentence.split()))\n",
    "\n",
    "output_label = tagger.tag(sent2features(feature_construct(sentence)))\n",
    "output_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\widi arsa\\AppData\\Local\\Temp\\ipykernel_14980\\1659254594.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('edge_scorer.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ngalih', 'gegaene'),\n",
       " ('gegaene', 'daya'),\n",
       " ('gegaene', 'apanga'),\n",
       " ('gegaene', 'aluhan'),\n",
       " ('gegaene', 'Nangingke'),\n",
       " ('gegaene', 'ia')]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_score = list(mst_parser(sentence).edges)\n",
    "edge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cek(sentence):\n",
    "    pos_sentence = tagger_pos.tag(sent2features_pos(sentence.split()))\n",
    "    output_label = tagger.tag(sent2features(feature_construct(sentence, pos_sentence)))\n",
    "    head_features = construct_head(sentence, sentence.split())\n",
    "    word, head = zip(*head_features)\n",
    "    return list(zip(word, head, output_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\widi arsa\\AppData\\Local\\Temp\\ipykernel_14980\\1659254594.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('edge_scorer.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Pan', 1, 'obj'),\n",
       " ('Karsa', 7, 'subj'),\n",
       " ('ajaka', 1, 'conj'),\n",
       " ('pianakne', 1, 'xcomp'),\n",
       " ('muani', 1, 'obj'),\n",
       " ('nanggap', 1, 'obj'),\n",
       " ('upah', 1, 'subj'),\n",
       " ('ngae', 0, 'root'),\n",
       " ('semer', 1, 'obj'),\n",
       " ('di', 1, 'case'),\n",
       " ('sisin', 1, 'obj'),\n",
       " ('rurunge', 1, 'nmod'),\n",
       " ('gede', 1, 'amod')]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cek(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('dataset/data_new.xlsx')\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_feature = [[]]\n",
    "\n",
    "temp_index = 0\n",
    "\n",
    "for index, value in df.iterrows():\n",
    "    if index != len(df)-1 and df['sentence_id'][index] != df['sentence_id'][index+1]:\n",
    "        raw_feature.append([])\n",
    "        temp_index += 1\n",
    "    # print(raw_feature)\n",
    "    if value['word'] != '.':\n",
    "        raw_feature[temp_index].append([value['word'], value['pos_tag'], value['head'], value['deprel']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\widi arsa\\AppData\\Local\\Temp\\ipykernel_14980\\1659254594.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('edge_scorer.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('Pan', 1, 'obj'),\n",
       "  ('Karsa', 7, 'subj'),\n",
       "  ('ajaka', 1, 'conj'),\n",
       "  ('pianakne', 1, 'xcomp'),\n",
       "  ('muani', 1, 'obj'),\n",
       "  ('nanggap', 1, 'obj'),\n",
       "  ('upah', 1, 'subj'),\n",
       "  ('ngae', 0, 'root'),\n",
       "  ('semer', 1, 'obj'),\n",
       "  ('di', 1, 'case'),\n",
       "  ('sisin', 1, 'obj'),\n",
       "  ('rurunge', 1, 'nmod'),\n",
       "  ('gede', 1, 'amod')],\n",
       " [['Pan', 'NNP', 2, 'comp'],\n",
       "  ['Karsa', 'NNP', 6, 'subj'],\n",
       "  ['ajaka', 'IN', 4, 'case'],\n",
       "  ['pianakne', 'NN', 6, 'subj'],\n",
       "  ['muani', 'NN', 4, 'appos'],\n",
       "  ['nanggap', 'VB', 0, 'root'],\n",
       "  ['upah', 'NN', 6, 'obj'],\n",
       "  ['ngae', 'VB', 6, 'xcomp'],\n",
       "  ['semer', 'NN', 8, 'obj'],\n",
       "  ['di', 'IN', 11, 'case'],\n",
       "  ['sisin', 'NN', 12, 'nmod'],\n",
       "  ['rurunge', 'NN', 8, 'nmod'],\n",
       "  ['gede', 'JJ', 12, 'amod']])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word, _, head, label = zip(*raw_feature[0])\n",
    "sents = \" \".join(word)\n",
    "cek(sents), raw_feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseval(y):\n",
    "    total = 0\n",
    "    true_head = 0\n",
    "    true_label = 0\n",
    "    true_dependensi = 0\n",
    "\n",
    "    for index in range(len(y)):\n",
    "        total += len(y[index])\n",
    "        word, _, head, label = zip(*y[index])\n",
    "        sent = \" \".join(word)\n",
    "        y_pred = cek(sent)\n",
    "        _, head_pred, label_pred = zip(*y_pred)\n",
    "        for i in range(len(head_pred)):\n",
    "            if head_pred[i] == head[i]:\n",
    "                true_head += 1\n",
    "            if label_pred[i] == label[i]:\n",
    "                true_label += 1\n",
    "            if head_pred[i] == head[i] and label_pred[i] == label[i]:\n",
    "                true_dependensi += 1\n",
    "\n",
    "    uas = true_head / total\n",
    "    las = true_label / total\n",
    "    exact = true_dependensi / total\n",
    "    return uas, las, exact, true_head, true_label, true_dependensi, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.60000000000001"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(raw_feature)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\widi arsa\\AppData\\Local\\Temp\\ipykernel_14980\\1659254594.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('edge_scorer.pth'))\n"
     ]
    }
   ],
   "source": [
    "test_count = round(len(raw_feature)*0.8)\n",
    "test_feature = raw_feature[test_count:]\n",
    "uas, las, exact, true_head, true_label, true_dependensi, total = parseval(raw_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.049403747870528106, 87, 1761)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uas, true_head, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4684838160136286, 825, 1761)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las, true_label, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03180011357183418, 56, 1761)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact, true_dependensi, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\widi arsa\\AppData\\Local\\Temp\\ipykernel_14980\\1659254594.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('edge_scorer.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Nangingke', 5],\n",
       " ['ia', 5],\n",
       " ['ngalih', 0],\n",
       " ['daya', 5],\n",
       " ['apanga', 5],\n",
       " ['gegaene', 2],\n",
       " ['aluhan', 5]]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_features = construct_head(sentence, sentence.split())\n",
    "word, head = zip(*head_features)\n",
    "head_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nmod', 'Nangingke'),\n",
       " ('subj', 'ia'),\n",
       " ('root', 'ngalih'),\n",
       " ('obj', 'daya'),\n",
       " ('iobj', 'apanga'),\n",
       " ('xcomp', 'gegaene'),\n",
       " ('obj', 'aluhan')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = sentence.split()\n",
    "map_output = list(zip(output_label, sentence_list))\n",
    "map_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_displacy(head_features, edge_label):\n",
    "    displacy_words = []\n",
    "    displacy_arcs = []\n",
    "\n",
    "    words, heads = zip(*head_features)\n",
    "    labels = edge_label\n",
    "    for index in range(len(heads)):\n",
    "        if heads[index] != 0:\n",
    "            dir = \"left\" if index > heads[index] else \"right\"\n",
    "            displacy_arcs.append({\n",
    "                \"start\" : index,\n",
    "                \"end\" : heads[index],\n",
    "                \"label\" : str(labels[index]),\n",
    "                \"dir\" : str(dir)\n",
    "            })\n",
    "        displacy_words.append({\n",
    "                \"text\" : str(words[index]),\n",
    "                \"tag\" : str(pos_sentence[index])\n",
    "            })\n",
    "    return {\n",
    "        \"words\" : displacy_words,\n",
    "        \"arcs\" : displacy_arcs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': [{'text': 'Nangingke', 'tag': 'NN'},\n",
       "  {'text': 'ia', 'tag': 'PR'},\n",
       "  {'text': 'ngalih', 'tag': 'VB'},\n",
       "  {'text': 'daya', 'tag': 'NN'},\n",
       "  {'text': 'apanga', 'tag': 'Z'},\n",
       "  {'text': 'gegaene', 'tag': 'VB'},\n",
       "  {'text': 'aluhan', 'tag': 'NN'}],\n",
       " 'arcs': [{'start': 0, 'end': 5, 'label': 'nmod', 'dir': 'right'},\n",
       "  {'start': 1, 'end': 5, 'label': 'subj', 'dir': 'right'},\n",
       "  {'start': 3, 'end': 5, 'label': 'obj', 'dir': 'right'},\n",
       "  {'start': 4, 'end': 5, 'label': 'iobj', 'dir': 'right'},\n",
       "  {'start': 5, 'end': 2, 'label': 'xcomp', 'dir': 'left'},\n",
       "  {'start': 6, 'end': 5, 'label': 'obj', 'dir': 'left'}]}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = construct_displacy(head_features, output_label)\n",
    "# displacy.render(data_1, style=\"dep\", manual=True, jupyter=True)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[0;32m      3\u001b[0m data_1 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[0;32m      5\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNangingke\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     ]\n\u001b[0;32m     21\u001b[0m }\n\u001b[1;32m---> 23\u001b[0m \u001b[43mdisplacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjupyter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\widi arsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\displacy\\__init__.py:64\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ments\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m     63\u001b[0m             doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ments\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ments\u001b[39m\u001b[38;5;124m\"\u001b[39m], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m], x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m---> 64\u001b[0m _html[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminify\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     65\u001b[0m html \u001b[38;5;241m=\u001b[39m _html[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RENDER_WRAPPER \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\widi arsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\displacy\\render.py:302\u001b[0m, in \u001b[0;36mDependencyRenderer.render\u001b[1;34m(self, parsed, page, minify)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_LANG)\n\u001b[0;32m    301\u001b[0m     render_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m     svg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_svg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marcs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m     rendered\u001b[38;5;241m.\u001b[39mappend(svg)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m page:\n",
      "File \u001b[1;32mc:\\Users\\widi arsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\displacy\\render.py:328\u001b[0m, in \u001b[0;36mDependencyRenderer.render_svg\u001b[1;34m(self, render_id, words, arcs)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender_svg\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     render_id: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    318\u001b[0m     words: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[0;32m    319\u001b[0m     arcs: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;124;03m\"\"\"Render SVG.\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m    render_id (Union[int, str]): Unique ID, typically index of document.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    RETURNS (str): Rendered SVG markup.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_levels\u001b[49m\u001b[43m(\u001b[49m\u001b[43marcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhighest_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevels\u001b[38;5;241m.\u001b[39mvalues(), default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhighest_level \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marrow_stroke\n",
      "File \u001b[1;32mc:\\Users\\widi arsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\displacy\\render.py:461\u001b[0m, in \u001b[0;36mDependencyRenderer.get_levels\u001b[1;34m(self, arcs)\u001b[0m\n\u001b[0;32m    459\u001b[0m levels \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(arcs, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m arc: arc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m arc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m--> 461\u001b[0m     level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_level\u001b[49m\u001b[43m[\u001b[49m\u001b[43marc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43marc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(arc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m], arc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    463\u001b[0m         max_level[i] \u001b[38;5;241m=\u001b[39m level\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "data_1 = {\n",
    "    'words': [\n",
    "        {'text': 'Nangingke', 'tag': 'NN'},\n",
    "        {'text': 'ia', 'tag': 'PR'},\n",
    "        {'text': 'ngalih', 'tag': 'VB'},\n",
    "        {'text': 'daya', 'tag': 'NN'},\n",
    "        {'text': 'apanga', 'tag': 'Z'},\n",
    "        {'text': 'gegaene', 'tag': 'VB'},\n",
    "        {'text': 'aluhan', 'tag': 'NN'}\n",
    "    ],\n",
    "    'arcs': [\n",
    "        {'start': 0, 'end': 5, 'label': 'nmod', 'dir': 'right'},\n",
    "        {'start': 1, 'end': 5, 'label': 'subj', 'dir': 'right'},\n",
    "        {'start': 3, 'end': 5, 'label': 'obj', 'dir': 'right'},\n",
    "        {'start': 4, 'end': 5, 'label': 'iobj', 'dir': 'right'},\n",
    "        {'start': 5, 'end': 2, 'label': 'xcomp', 'dir': 'left'},\n",
    "        {'start': 6, 'end': 5, 'label': 'obj', 'dir': 'left'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "displacy.render(data_1, style=\"dep\", manual=True, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6e0104e811c149cd983b5b2d3095a61a-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apeteng</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">eats</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6e0104e811c149cd983b5b2d3095a61a-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6e0104e811c149cd983b5b2d3095a61a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M220.0,179.0 L228.0,167.0 212.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6e0104e811c149cd983b5b2d3095a61a-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6e0104e811c149cd983b5b2d3095a61a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6e0104e811c149cd983b5b2d3095a61a-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6e0104e811c149cd983b5b2d3095a61a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Data custom\n",
    "data = {\n",
    "    \"words\": [\n",
    "        {\"text\": \"Apeteng\", \"tag\": \"NNP\"},\n",
    "        {\"text\": \"eats\", \"tag\": \"VBZ\"},\n",
    "        {\"text\": \"an\", \"tag\": \"DT\"},\n",
    "        {\"text\": \"apple\", \"tag\": \"NN\"},\n",
    "    ],\n",
    "    \"arcs\": [\n",
    "        {\"start\": 0, \"end\": 1, \"label\": \"nsubj\", \"dir\": \"right\"},\n",
    "        {\"start\": 1, \"end\": 3, \"label\": \"obj\", \"dir\": \"right\"},\n",
    "        {\"start\": 2, \"end\": 3, \"label\": \"det\", \"dir\": \"left\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Visualisasi dependensi\n",
    "displacy.render(data, style=\"dep\", manual=True, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (4.23.3)\n",
      "Requirement already satisfied: requests in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (3.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (2.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (2022.9.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\widi arsa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.1)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 1.1/1.1 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 586.9/586.9 kB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.14.0 stanza-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza.utils.visualize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[276], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualize_parse\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Membuat data dalam format Stanza\u001b[39;00m\n\u001b[0;32m      5\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNangingke\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngalih\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaya\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapanga\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgegaene\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maluhan\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stanza.utils.visualize'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from stanza.utils.visualize import visualize_parse\n",
    "\n",
    "# Membuat data dalam format Stanza\n",
    "words = ['Nangingke', 'ia', 'ngalih', 'daya', 'apanga', 'gegaene', 'aluhan']\n",
    "dependencies = [\n",
    "    (0, 5, 'nmod'), (1, 5, 'subj'), (3, 5, 'obj'), (4, 5, 'iobj'),\n",
    "    (5, 2, 'xcomp'), (6, 5, 'obj')\n",
    "]\n",
    "\n",
    "# Membuat pipeline Stanza untuk bahasa Indonesia\n",
    "nlp = stanza.Pipeline('id')\n",
    "\n",
    "# Membuat doc kosong untuk diisi\n",
    "doc = stanza.models.common.doc.Document()\n",
    "\n",
    "# Menambahkan kata-kata ke dalam doc\n",
    "for word in words:\n",
    "    doc.sentences.append(stanza.models.common.doc.Sentence([stanza.models.common.doc.Word(word)]))\n",
    "\n",
    "# Menambahkan dependensi ke dalam doc (menyesuaikan format)\n",
    "for dep in dependencies:\n",
    "    doc.sentences[dep[1]].words[0].head = doc.sentences[dep[0]].words[0]\n",
    "    doc.sentences[dep[1]].words[0].deprel = dep[2]\n",
    "\n",
    "# Visualisasi\n",
    "visualize_parse(doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
